\section{Introduction}

% Paragraph 1: Task Definition - Emphasizing "Closed-Set Ranking"
The domain of Question Answering (QA), specifically the AI2 Reasoning Challenge (ARC), requires reasoning capabilities that extend beyond simple fact retrieval. Unlike open-ended generation tasks, the evaluation protocol for ARC operates as a {closed-set ranking task}. The model is not required to generate a solution from scratch but must strictly assign the highest likelihood to the correct option among a constrained set of candidates. Consequently, the primary objective is to accurately {discriminate} the ground truth from distractors rather than prioritizing global generative fluency.

% Paragraph 2: Challenges - From "SFT Limits" to "Objective Mismatch"
Standard Supervised Fine-Tuning (SFT) encounters a structural {objective mismatch} in this context. SFT optimizes the probability distribution over the entire global vocabulary. This process allocates significant parameter capacity to global correction, such as suppressing formatting artifacts, rather than sharpening the decision boundary between candidates. This inefficiency explains why injecting reasoning traces, as seen in SlimOrca, or incorporating heavy domain knowledge, like Mathstral, often yields diminishing returns. Lacking explicit negative feedback, SFT models prioritize syntactic adherence over the {discriminative logic} necessary to distinguish truth from high-quality distractors.

% Paragraph 3: Proposed Method - Introducing DRO
We address these challenges under Parameter-Efficient Fine-Tuning (PEFT) constraints by proposing {Discriminative Ranking Optimization (DRO)}. This methodology shifts the training paradigm from generation to discrimination by minimizing the optimization search space. We first implement an {Evaluation-Aware Template Strategy} that aligns training inputs with the evaluation protocol to freeze the syntactic search space. Additionally, we utilize intrinsic distractors from the dataset as hard negatives. The model is trained to strictly maximize the {relative margin} between the correct answer and the distractors, ensuring that limited parameters are dedicated exclusively to the ranking objective.

% Paragraph 4: Results & Mechanism Transfer
We empirically validate the efficacy of DRO through extensive experiments. Our method achieved a substantial performance improvement on the ARC-Challenge, increasing accuracy from a zero-shot baseline of 61.43\% to {74.40\%}. Beyond task-specific gains, we present evidence of {mechanism transfer}. A model trained exclusively on the unrelated {CommonsenseQA} dataset achieved performance on ARC comparable to a model trained on the target data itself. This result suggests that DRO equips the model with a {fundamental discriminative mechanism}, or a generalized skill for multiple-choice reasoning, rather than merely overfitting to domain-specific knowledge.
% % Paragraph 1: Context & Task Definition
% The domain of Scientific Question Answering (QA), specifically the ARC (AI2 Reasoning Challenge) benchmark, requires models to perform complex reasoning beyond simple fact retrieval. However, contrary to open-ended generation tasks, the evaluation protocol for ARC is fundamentally a {ranking task}: the model must assign the highest likelihood to the correct option among multiple choices. Consequently, the core objective of training is not merely to generate coherent text, but to accurately {discriminate} the correct answer from incorrect ones based on likelihood scores.

% % Paragraph 2: The Challenges (SFT Limits, LIMA, Format Bias)
% However, standard Supervised Fine-Tuning (SFT) faces structural limitations in fostering this discriminative capability. Recent studies, such as the LIMA hypothesis, suggest that SFT primarily functions as ``surface form alignment'' rather than injecting new knowledge or significantly enhancing reasoning capabilities. This limitation is exacerbated in Multiple-Choice Question (MCQ) tasks, where the evaluation relies on single-token likelihoods without the intermediate supervision of reasoning steps (Chain-of-Thought). Lacking explicit negative feedback, SFT models often struggle to distinguish correct logic from {plausible distractors} and exhibit brittle performance due to {template sensitivity} and {positional bias}. Thus, simply maximizing the probability of the ground truth is insufficient to sharpen the decision boundary against high-quality distractors.

% % Paragraph 3: Proposed Method (Strategic Alignment & Evaluation-Aware Template)
% To address these challenges under the constraints of limited compute (PEFT), we propose a {``Strategic Alignment''} methodology that shifts the training objective from generation to discrimination. First, we employ a {Preference Optimization (specifically DPO/SimPO)} framework tailored for MCQs. By utilizing the dataset's intrinsic distractors as ``Hard Negative'' signals, we train the model to explicitly maximize the margin between scientific truth and plausible falsehoods. Crucially, we implement an {Evaluation-Aware Template Strategy}. Unlike standard SFT which imposes an arbitrary format, we align our training templates exactly with the downstream evaluation protocol. This design minimizes the ``alignment tax,'' ensuring that the limited parameter budget of PEFT is allocated exclusively to sharpening the {discriminative boundary} rather than adapting to surface-level syntax.

% % Paragraph 4: Results & Generalization (ARC + CommonsenseQA)
% We empirically validate the efficacy of our approach through extensive experiments. Our method achieved a substantial performance leap on the ARC-Challenge, elevating the accuracy from a baseline of 61.43\% to {74.xx\%}. Beyond task-specific improvements, we present compelling evidence of the model's {generalization capabilities}. Despite being trained \textit{exclusively} on the ARC-Challenge training set, our model demonstrated remarkable transfer learning effects on other multiple-choice benchmarks. Notably, on {CommonsenseQA}, accuracy surged from 59.3\% to {73.5\%} without any direct exposure to the dataset. This result strongly suggests that our approach equips the model with a {fundamental discriminative mechanism} robust enough to handle unseen multiple-choice tasks, rather than merely overfitting to domain-specific knowledge.