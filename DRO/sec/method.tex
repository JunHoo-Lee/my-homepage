% \section{Method}

% \subsection{Pipeline?}

% 여기서 할 말은 뭐냐면, 우리가 뭘 사용할거고, 그래서 왜 이것만 가능한지. 그러니까, RAG 쓸수도 있을거고, 아니면 고전적으로 여러개 뽑아서 voting 할수도 있을거고, 아니면 reasoning할 수도 있을거고.. 여러가지 방법 가능할것같은데 실제로는 왜 그냥 peft만 했는지. 왜냐하면 실제로는 표준화된 pipeline에서 검증을 할 거고, (왜냐하면 이 말을 왜 하는거냐면, 지금 업스테이지 인턴중인건데 여기서 하는게 foundational model에서 성능뽑는게 중요함) 그럼 그런 트릭같은걸 쓰는게 의미가 없다.

% \subsection{Challenges}

% 여기서 할 말은 뭐냐면, Scientific QA, 그냥 multiple QA 세팅에서의 문제점들. 그리고 실제로 SFT를 통해서 얻을 것. 그러니까 SFT signal을 준다고 해보자. 이떄 문제가 뭐냐면.. 그니까 그 답 자체가 만약에 format일수도 있으니까. 그 말은 뭐냐면 그냥 회문이어서. 아예 이상한 답을 할 수도 있고... 이러니까. 이 경우엔 어떤 템플릿 이해못하기? 영역일수도 있어서 애매하다.

% 일단 내가 본게 뭐냐면, 처음에는 reasoning 에서는 그러면 보통 reasoning 능력 늘려주면 좋다고 하니까 찾아본게 뭐냐면, slimorca. 그게 뭐냐면, reasoning tradce를 gpt4 에서 만들어놓고, 그것들 가지고 fine-tune 한 모델. 그 경우에 arc score가  62.54. 그렇다면, 이제 문제는 어떻게 바뀌는거냐면, 사실 reasoning 자체가 성능을 엄청나게 올려주지는 않는다. 적어도. 그 내 가용 자원 상에서는 그렇다. 왜냐하면, slimorca가 제대로 된 distillation data, 그리고 Full fine tuning. 그렇다면 이거 이상으로 reasoning을 늘리기는 어렵다.
% 추가적인 observation이. 그렇다면 과연 이론상으로 엄청나게 수학쪽으로 학습된 mathtral은 어떨까? 그거마저도 58.64. 오히려 이 경우는 성능이 더 떨어졌다. 여기서 잠정적으로 내린 결론은, 나이브하게 저런 세팅에서는 하면 안된다는 거.

% 이는 왜 그런거냐면, 우리의 가설은, 실제로 과학을 더 학습시키는 건 불가능하고. 이때 해야하는거는 그냥 주어진 train data의 구조 자체를 잘 활용하는 거가 최선. 즉 그냥 객관식 문제에 대해서 더 잘 답하는 문제로 변환하기. 즉. 그냥 주어진 Q,A가 있으면. 여러개의 choice 중에 더 강한 choice만 주도록 그냥 weight 주기. 즉 내가 하는거는, 가용가능한 weight pool에서, 애초에 search space를 줄여서 유의미한 학습공간이 가능하도록 하는 것.즉, 포맷팅 이런 류의 공간은 보지 않겠다.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{Illustration of the Objective Mismatch. The standard next-token prediction objective (left) requires the model to suppress irrelevant global vocabulary tokens, whereas the ARC protocol (right) only necessitates relative likelihood comparisons between predefined candidates. We hypothesize that this discrepancy causes inefficient allocation of model capacity, hindering performance on closed-set tasks.}
    \label{fig:placeholder}
\end{figure}
\begin{table}[ht]
\centering
\caption{Comparison of model performance on the ARC-Challenge benchmark. Despite reasoning-heavy or domain-specific fine-tuning, improvements remain marginal or negative compared to the base model.}
\label{tab:arc_performance}
\begin{tabular}{llc}
\toprule
\textbf{Model} & \textbf{Key Characteristics} & \textbf{ARC-C (Acc, \%)} \\ \midrule
\textbf{Mistral-7B} & General-purpose base model & \textbf{61.43} \\
SlimOrca & Fine-tuned on high-quality reasoning traces (SFT) & 62.64 \\
Mathstral & Specialized in STEM domains; Heavy knowledge injection & 58.64 \\ \bottomrule
\end{tabular}
\end{table}
\section{Methodology}

\subsection{Experimental Scope}
Our primary objective aligns with the fundamental goal of foundation model development\textbf{(독파모)}, which is to enhance the {intrinsic reasoning capabilities} of the model without reliance on external augmentations. Assuming that evaluations are conducted through standardized protocols, we strictly exclude inference-time techniques such as Retrieval-Augmented Generation (RAG) or Majority Voting ensembles. These methods are typically incompatible with standard evaluation pipelines and can obscure the standalone performance of the model. Therefore, we focus on optimizing the model weights under strict resource constraints. We utilize {PEFT (LoRA)} to demonstrate that performance gains are derived from {efficient alignment strategies} rather than massive compute scaling.

\subsection{Challenges: The Objective Mismatch in SFT}
Before designing our approach, we analyzed the structural limitations of standard Supervised Fine-Tuning (SFT) in the context of Multiple-Choice Question (MCQ) tasks. Our empirical observations from preliminary experiments suggest a fundamental {misalignment between the training objective and the evaluation metric}.

\paragraph{Limitations of Reasoning Injection}
One might hypothesize that the performance bottleneck stems from a lack of reasoning capability. If this were true, supervised fine-tuning on high-quality reasoning traces should yield significant improvements. However, \textit{SlimOrca}, which was fine-tuned on reasoning data with GPT-4, showed similar returns compared to the baseline. This suggests that merely exposing the model to reasoning steps via SFT is insufficient to enhance performance on discriminative benchmarks.

\paragraph{Inefficacy of Domain-Specific SFT}
Alternatively, if the limitation were a lack of domain knowledge, models specialized in scientific domains should outperform generalist models. However, \textit{Mathstral}, an architecture heavily fine-tuned for STEM tasks, underperformed compared to our base model, \textit{Mistral}. This indicates that naive knowledge injection can lead to negative transfer, where domain specialization compromises general commonsense reasoning.

\paragraph{Global Correction vs. Local Ranking}
The standard SFT objective (Next-Token Prediction) optimizes the probability distribution over the entire global vocabulary ($|V| \approx 32k+$). In this regime, a significant portion of the parameter capacity is allocated to {behavioral correction}. This involves suppressing non-compliant tokens, such as junk words or formatting artifacts like `[EOS]` and `\textbackslash n`, to enforce syntactic adherence.

Consider a concrete example from the dataset: \textit{"George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?"} with candidates \{ \textit{Palms, Head, Heart} \}.
Under the SFT objective, the model creates gradients to maximize the probability of the token \textit{"Palms"} while simultaneously suppressing thousands of irrelevant tokens such as \texttt{[}, \texttt{Template:}, or \texttt{Answer:}. 
However, the ARC evaluation protocol is strictly a {closed-set ranking task}. The metric is agnostic to whether $P(\textit{Palms}) > P(\texttt{[EOS]})$; it solely requires that $P(\textit{Palms}) > P(\textit{Head})$ and $P(\textit{Palms}) > P(\textit{Heart})$.

In this context, optimizing the model to "speak correctly" by suppressing global vocabulary noise represents a computational inefficiency. 


\subsection{Discriminative Ranking Optimization}

To bypass the computational inefficiency of global generative correction, we propose a strategy of \textbf{Discriminative Ranking Optimization (DRO)}. This approach strictly optimizes the {relative margin} within the candidate set, effectively disregarding irrelevant global generative artifacts. We implement this through two key mechanisms:

\begin{itemize}
    \item \textbf{Syntactic Constraint (Template Alignment):} We align the training templates exactly with the downstream evaluation protocol. This effectively freezes the syntactic search space and removes the necessity for the model to learn arbitrary instruction formats. This strategy mitigates the "alignment tax," preventing the model from forgetting knowledge while adapting to new syntax.
    
    \item \textbf{Local-Ranking Based Optimization:} Instead of optimizing for global probability across the entire vocabulary, we utilize the dataset's intrinsic distractors to explicitly construct negative pairs. This redefines the loss landscape: the objective is strictly to maximize the {relative likelihood margin} between the correct option and the distractors. This ensures the model learns to rank the valid answer higher than plausible falsehoods within the closed candidate set, without wasting capacity on generative fluency.
\end{itemize}

By decoupling syntax from semantics, we ensure that every gradient update contributes solely to {sharpening the discriminative boundary} within the candidate set. This maximizes the efficiency of the limited trainable parameters available in the PEFT setting.

\paragraph{Instantiation}
We instantiated the proposed strategy with specific design choices to maximize efficiency under the PEFT constraint. 
First, to enforce the {Syntactic Constraint}, we pre-processed the training dataset to strictly mirror the prompt templates used in the \texttt{lm-evaluation-harness}. This ensures zero distribution shift between the training and evaluation phases. 
Second, for the {Local-Ranking Based Optimization}, we adopted {Simple Preference Optimization (SimPO)}, a reference-free variant of Contrastive Preference Optimization (CPO). We selected SimPO over standard DPO because it directly enforces a target reward margin without the computational overhead of a reference model, aligning perfectly with our resource-constrained setting. 
Finally, the preference pairs $(y_{chosen}, y_{rejected})$ were explicitly constructed by assigning the ground truth option as the chosen response and the provided {distractors} as the rejected responses, thereby training the model to discriminate against the specific pitfalls designed by the dataset authors.
% \subsection{Format Aware Policy Optimization}
% \section{Methodology}

% \subsection{Experimental Scope: Focusing on Intrinsic Capabilities}
% Our primary objective aligns with the \textbf{fundamental goal of foundation model development}(독파모): enhancing the model's {intrinsic reasoning capabilities} without reliance on external augmentations. assuming the evaluattions are measured through 규격화된 protocol. Consequently, we exclude inference-time techniques such as Retrieval-Augmented Generation (RAG) or Majority Voting ensembles. While these methods can artificially inflate scores, these methods are cannot used in these protocol. Therefoe, we focus on optimizing the model weights under resource constraint, utilizing \textbf{PEFT (LoRA)} to demonstrate that performance gains are derived from {efficient alignment strategies} rather than massive compute scaling.

% \subsection{Challenges: The Objective Mismatch in SFT}
% Before designing our approach, we analyzed the structural limitations of standard Supervised Fine-Tuning (SFT) in the context of Multiple-Choice Question (MCQ) tasks. Our empirical observations from preliminary experiments—where \textit{SlimOrca} (Reasoning SFT) and \textit{Mathstral} (Domain SFT) yielded diminishing returns or negative transfer—suggest a fundamental \textbf{misalignment between the training objective and the evaluation metric}.

% \paragraph{Reasoning Does not Help} 만약 우리가 이 문제가 추론능력ㅇ르 향상시켜서 해결될 수 있을거라 생각한다면, reasoning 능력을 늘려서 해결될 수 있을거라 생각할 수 이ㅓㅅ다.

% \paragraph{Massive SFT does not help} mathstral은 scientific discovery 문제를 풀기위해서 STEM 잘풀기 위해서 파인튠 많이 된 아키텍쳐다. 만약에 추가 지식을 집어넣는게 효과적이라면, mathstral, 즉 우리의 base model인 mistral에서 finetune된게 성능이 좋아야 한다.

% \paragraph{MCQ is local ranking problem} 
% The standard SFT objective (Next-Token Prediction) optimizes the probability distribution over the entire global vocabulary ($|V| \approx 32k+$). In this regime, a significant portion of the parameter capacity is allocated to \textbf{behavioral correction}—suppressing non-compliant tokens (e.g., junk words, formatting artifacts like `[EOS]` or `\n`) to enforce syntactic adherence.

% Consider a concrete example from the dataset: \textit{"George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?"} with candidates \{ \textit{Palms, Head, Heart} \}.
% Under the SFT objective, the model creates gradients to maximize the probability of the token \textit{"Palms"} while simultaneously suppressing thousands of irrelevant tokens such as \texttt{[}, \texttt{Template:}, or \texttt{Answer:}. 
% However, the ARC evaluation protocol is strictly a \textbf{closed-set ranking task}. The metric is agnostic to whether $P(\textit{Palms}) > P(\texttt{[EOS]})$; it solely requires that $P(\textit{Palms}) > P(\textit{Head})$ and $P(\textit{Palms}) > P(\textit{Heart})$. 

% In this context, optimizing the model to "speak correctly" by suppressing global vocabulary noise represents a computational inefficiency. Our strategy bypasses this by utilizing DPO to strictly optimize the \textbf{relative margin} within the candidate set, ignoring irrelevant global generative artifacts.


% \begin{itemize}
%     \item \textbf{Syntactic Constraint (Template Alignment):} We align the training templates exactly with the downstream evaluation protocol. This effectively "freezes" the syntactic search space, removing the need for the model to learn arbitrary instruction formats. This prevents the alignment tax where the model forgets knowledge while adapting to new syntax.
    
%     \item \textbf{Discriminative Ranking Optimization:} Instead of SFT, we employ Direct Preference Optimization (DPO) utilizing the dataset's intrinsic distractors as negative pairs. This redefines the loss landscape: rather than maximizing the absolute probability of the target token against the entire vocabulary, the objective is strictly to maximize the \textbf{relative margin} between the correct option and the distractors.
% \end{itemize}

% By decoupling syntax from semantics, we ensure that every gradient update contributes solely to \textbf{sharpening the discriminative boundary} within the candidate set, thereby maximizing the efficiency of the limited trainable parameters.