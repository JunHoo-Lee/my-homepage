\section{Related Work}

\paragraph{Instruction Tuning for Reasoning}
Enhancing the reasoning capabilities of Large Language Models (LLMs) has primarily relied on Supervised Fine-Tuning (SFT) with high-quality demonstration data. Methods like SlimOrca or Mathstral utilize synthetic reasoning traces (e.g., Chain-of-Thought) generated by stronger models to guide smaller models. While these approaches improve general instruction following and generative coherence, they inherently treat the task as open-ended generation. As highlighted in our analysis, this generative objective often misaligns with the strict ranking nature of multiple-choice evaluations, leading to suboptimal parameter efficiency and potential negative transfer when domain-specific data is over-utilized.

\paragraph{Preference Learning in Reasoning Tasks}
Recent advancements have shifted towards Preference Optimization methods, such as RLHF and DPO, to align models with human intent. In the context of reasoning, several studies have applied DPO to improve performance on benchmarks like MMLU. However, the prevailing paradigm in these works is to optimize the quality of \textit{intermediate reasoning steps} or generative explanations. Improvements in multiple-choice accuracy are typically reported as auxiliary benefits or {by-products} of enhanced reasoning capabilities. These methods still operate under a generative framework, aiming to produce better text rather than explicitly sharpening the decision boundary between pre-defined options.

\paragraph{Direct Optimization for Multiple-Choice QA}
Our work diverges from the aforementioned approaches by re-framing Scientific QA not as a reasoning generation problem, but as a {discriminative ranking problem}. Unlike methods that rely on generated negatives or generic preference pairs, we leverage the intrinsic distractors provided within the dataset to construct hard-negative pairs. This allows us to apply reference-free preference optimization (SimPO) to directly target the relative margin between the correct answer and plausible falsehoods. \textbf{To the best of our knowledge, this is the first work to explicitly decouple the discriminative ranking objective from generative reasoning in the context of scientific QA, demonstrating that mechanism transfer can be achieved without domain-specific knowledge injection.}